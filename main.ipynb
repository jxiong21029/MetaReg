{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from flax.training import train_state\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class RunningMoments:\n",
    "    \"\"\"\n",
    "    Tracks running mean and variance\n",
    "    Adapted from github.com/MadryLab/implementation-matters, which took it from\n",
    "    github.com/joschu/modular_rl. Math in johndcook.com/blog/standard_deviation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.m = 0\n",
    "        self.s = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        assert isinstance(x, float) or isinstance(x, int)\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.m = x\n",
    "        else:\n",
    "            old_m = self.m\n",
    "            self.m = old_m + (x - old_m) / self.n\n",
    "            self.s = self.s + (x - old_m) * (x - self.m)\n",
    "\n",
    "    def mean(self):\n",
    "        return self.m\n",
    "\n",
    "    def std(self):\n",
    "        if self.n > 1:\n",
    "            return math.sqrt(self.s / (self.n - 1))\n",
    "        else:\n",
    "            return self.m\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self._buffer_data = defaultdict(RunningMoments)\n",
    "        self.cumulative_data = defaultdict(list)\n",
    "        self.seen_plot_directories = set()\n",
    "\n",
    "    # log metrics, used once per epoch\n",
    "    def log(self, metrics=None, **kwargs):\n",
    "        metrics = {} if metrics is None else metrics\n",
    "        for k, v in {**metrics, **kwargs}.items():\n",
    "            self.cumulative_data[k].append(v)\n",
    "\n",
    "    # push metrics logged many times per epoch, e.g. loss, means and stds computed\n",
    "    def push(self, metrics=None, **kwargs):\n",
    "        metrics = {} if metrics is None else metrics\n",
    "        for k, v in {**metrics, **kwargs}.items():\n",
    "            self._buffer_data[k].push(v)\n",
    "\n",
    "    def step(self):\n",
    "        for k, v in self._buffer_data.items():\n",
    "            self.cumulative_data[k + \"_mean\"].append(v.mean())\n",
    "            self.cumulative_data[k + \"_std\"].append(v.std())\n",
    "        self._buffer_data.clear()\n",
    "\n",
    "    def tune_report(self):\n",
    "        from ray import tune\n",
    "\n",
    "        tune.report(**{k: v[-1] for k, v in self.cumulative_data.items()})\n",
    "\n",
    "    def air_report(self, **kwargs):\n",
    "        from ray.air import session\n",
    "\n",
    "        session.report({k: v[-1] for k, v in self.cumulative_data.items()}, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        if not filename.endswith(\".pickle\"):\n",
    "            filename = filename + \".pickle\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def generate_plots(self, dirname=\"plotgen\"):\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        matplotlib.use(\"Agg\")\n",
    "        sns.set_theme()\n",
    "\n",
    "        if dirname not in self.seen_plot_directories:\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "            for filename in os.listdir(dirname):\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            self.seen_plot_directories.add(dirname)\n",
    "\n",
    "        for k, v in self.cumulative_data.items():\n",
    "            if k.endswith(\"_std\"):\n",
    "                continue\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            x = np.arange(len(self.cumulative_data[k]))\n",
    "            v = np.array(v)\n",
    "            if k.endswith(\"_mean\"):\n",
    "                name = k[:-5]\n",
    "\n",
    "                (line,) = ax.plot(x, v, label=k)\n",
    "                stds = np.array(self.cumulative_data[name + \"_std\"])\n",
    "                ax.fill_between(\n",
    "                    x, v - stds, v + stds, color=line.get_color(), alpha=0.15\n",
    "                )\n",
    "            else:\n",
    "                name = k\n",
    "                (line,) = ax.plot(x, v)\n",
    "            ax.scatter(x, v, color=line.get_color())\n",
    "\n",
    "            fig.suptitle(name)\n",
    "            fig.savefig(os.path.join(dirname, name))\n",
    "            plt.close(fig)\n",
    "\n",
    "    def convergence(self, key, p=0.98):\n",
    "        \"\"\"Estimates the degree to which some metric has converged.\n",
    "        A custom metric by me (Jerry). Close to zero when the metric is clearly trending\n",
    "        upwards or downwards, close to one when changes in the metric seem to be\n",
    "        dominated by noise. Intended for debugging purposes, not for scientific usage.\n",
    "        p controls the degree to which this metric weights recently measured values.\n",
    "        p = 0 results in a uniform weighting, independent of time. More and more weight\n",
    "        is placed on the last few values as p approaches 1.\n",
    "        \"\"\"\n",
    "        assert key in self.cumulative_data\n",
    "\n",
    "        data = self.cumulative_data[key]\n",
    "        if len(data) <= 1:\n",
    "            return 0\n",
    "\n",
    "        diffs = np.array([data[i + 1] - data[i] for i in range(len(data) - 1)])\n",
    "        w = np.power((1 - p), (1 - np.linspace(0, 1, num=len(diffs))))\n",
    "        w = w / np.sum(w)\n",
    "\n",
    "        m = np.sum(w * diffs)\n",
    "        v = np.sum(w * diffs * diffs)\n",
    "\n",
    "        return 1 - abs(m) / math.sqrt(v + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "\n",
    "class CifarResnet(nn.Module):\n",
    "    n: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        x = nn.Conv(16, kernel_size=(3, 3))(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "\n",
    "        for _ in range(self.n):\n",
    "            out = nn.Conv(16, kernel_size=(3, 3))(x)\n",
    "            out = nn.relu(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            out = nn.Conv(16, kernel_size=(3, 3))(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            x = nn.relu(out + x)\n",
    "\n",
    "        for _ in range(self.n):\n",
    "            out = nn.Conv(32, kernel_size=(3, 3), strides=(2, 2))(x)\n",
    "            out = nn.relu(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            out = nn.Conv(32, kernel_size=(3, 3))(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            residual = nn.Conv(32, kernel_size=(3, 3), strides=(2, 2))(x)\n",
    "            x = nn.relu(out + residual)\n",
    "\n",
    "        for _ in range(self.n):\n",
    "            out = nn.Conv(64, kernel_size=(3, 3), strides=(2, 2))(x)\n",
    "            out = nn.relu(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            out = nn.Conv(64, kernel_size=(3, 3))(out)\n",
    "            out = nn.BatchNorm(use_running_average=not train)(out)\n",
    "            residual = nn.Conv(64, kernel_size=(3, 3), strides=(2, 2))(x)\n",
    "            x = nn.relu(out + residual)\n",
    "\n",
    "        x = x.mean(axis=(-2, -3))\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def loss_fn(params, ts, images, labels):\n",
    "    logits, updates = ts.apply_fn(\n",
    "        {\"params\": params, \"batch_stats\": ts.batch_stats},\n",
    "        images,\n",
    "        train=True,\n",
    "        mutable=[\"batch_stats\"],\n",
    "    )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "    return loss, (logits, updates)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def inner_step(ts: TrainState, images, labels):\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits, updates)), grads = grad_fn(ts.params, ts, images, labels)\n",
    "    ts = ts.apply_gradients(grads=grads)\n",
    "    ts = ts.replace(batch_stats=updates[\"batch_stats\"])\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": jnp.mean(jnp.argmax(logits, -1) == labels),\n",
    "    }\n",
    "    return ts, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(ts: TrainState, images, labels):\n",
    "    logits = ts.apply_fn(\n",
    "        {\"params\": ts.params, \"batch_stats\": ts.batch_stats}, images, train=False\n",
    "    )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": jnp.mean(jnp.argmax(logits, -1) == labels),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def standard_trial(rng, train_loader, test_loader, config):\n",
    "    model = CifarResnet(n=3)\n",
    "    variables = model.init(rng, np.zeros((32, 32, 3)), train=True)\n",
    "    ts = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=variables[\"params\"],\n",
    "        batch_stats=variables[\"batch_stats\"],\n",
    "        tx=optax.chain(\n",
    "            optax.add_decayed_weights(\n",
    "                weight_decay=config[\"weight_decay\"],\n",
    "                mask=jax.tree_map(lambda x: x.ndim != 1, variables[\"params\"]),\n",
    "            ),\n",
    "            optax.sgd(config[\"lr\"], momentum=0.9),\n",
    "        ),\n",
    "    )\n",
    "    logger = Logger()\n",
    "\n",
    "    for _ in range(100):\n",
    "        for images, labels in tqdm.tqdm(train_loader):\n",
    "            images = transforms.RandomCrop((32, 32), padding=4)(images)\n",
    "            images = np.asarray(images).transpose((0, 2, 3, 1))\n",
    "            labels = np.asarray(labels)\n",
    "\n",
    "            ts, train_metrics = inner_step(ts, images, labels)\n",
    "            train_metrics = jax.tree_map(lambda x: x.item(), train_metrics)\n",
    "            logger.push({\"train_\" + k: v for k, v in train_metrics.items()})\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = np.asarray(images).transpose((0, 2, 3, 1))\n",
    "            labels = np.asarray(labels)\n",
    "\n",
    "            test_metrics = eval_step(ts, images, labels)\n",
    "            test_metrics = jax.tree_map(lambda x: x.item(), test_metrics)\n",
    "            logger.push({\"test_\" + k: v for k, v in test_metrics.items()})\n",
    "\n",
    "        logger.step()\n",
    "        logger.generate_plots()\n",
    "\n",
    "\n",
    "def outer_loss(weight_decay, ts, images, labels, valid_images, valid_labels):\n",
    "    assert \"weight_decay\" in ts.opt_state.hyperparams\n",
    "    ts.opt_state.hyperparams[\"weight_decay\"] = weight_decay\n",
    "\n",
    "    ts = inner_step(ts, images, labels)\n",
    "\n",
    "    loss = loss_fn(ts.params, ts, valid_images, valid_labels)\n",
    "    return loss, ts\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def outer_step(weight_decay, ts: TrainState, images, labels, valid_images, valid_labels):\n",
    "    (loss, ts), grad = jax.value_and_grad(outer_loss, has_aux=True)(weight_decay, ts, images, labels, valid_images, valid_labels)\n",
    "    weight_decay = weight_decay - 0.01 * grad  # lol hardcoded lr whatever\n",
    "    return weight_decay, ts\n",
    "\n",
    "\n",
    "def meta_reg_trial(rng, train_loader, test_loader, config):\n",
    "    model = CifarResnet(n=3)\n",
    "    variables = model.init(rng, np.zeros((32, 32, 3)), train=True)\n",
    "    ts = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=variables[\"params\"],\n",
    "        batch_stats=variables[\"batch_stats\"],\n",
    "        tx=optax.chain(\n",
    "            optax.add_decayed_weights(\n",
    "                weight_decay=config[\"weight_decay\"],\n",
    "                mask=jax.tree_map(lambda x: x.ndim != 1, variables[\"params\"]),\n",
    "            ),\n",
    "            optax.sgd(config[\"lr\"], momentum=0.9),\n",
    "        ),\n",
    "    )\n",
    "    logger = Logger()\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    for _ in range(100):\n",
    "        for images, labels in tqdm.tqdm(train_loader):\n",
    "            images = transforms.RandomCrop((32, 32), padding=4)(images)\n",
    "            images = np.asarray(images).transpose((0, 2, 3, 1))\n",
    "            labels = np.asarray(labels)\n",
    "\n",
    "            ts, train_metrics = inner_step(ts, images, labels)\n",
    "            train_metrics = jax.tree_map(lambda x: x.item(), train_metrics)\n",
    "            logger.push({\"train_\" + k: v for k, v in train_metrics.items()})\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = np.asarray(images).transpose((0, 2, 3, 1))\n",
    "            labels = np.asarray(labels)\n",
    "\n",
    "            test_metrics = eval_step(ts, images, labels)\n",
    "            test_metrics = jax.tree_map(lambda x: x.item(), test_metrics)\n",
    "            logger.push({\"test_\" + k: v for k, v in test_metrics.items()})\n",
    "\n",
    "        logger.step()\n",
    "        logger.generate_plots()\n",
    "\n",
    "\n",
    "def main():\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"data/\", transform=torchvision.transforms.ToTensor(), download=True\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"data/\",\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=100, shuffle=True, drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100)\n",
    "    standard_trial(rng, train_loader, test_loader, {\"lr\": 1e-2, \"weight_decay\": 1e-4})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}